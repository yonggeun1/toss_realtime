import os
from dotenv import load_dotenv
from supabase import create_client, Client

# .env íŒŒì¼ ë¡œë“œ (ë¡œì»¬ ê°œë°œ í™˜ê²½ìš©)
load_dotenv()

# Supabase ì„¤ì • (ê¶Œí•œ ë¬¸ì œë¥¼ í”¼í•˜ê¸° ìœ„í•´ Service Role Keyì¸ 'Secret_keys'ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤)
url = os.getenv("Project_URL", "").strip()
key = os.getenv("Secret_keys", "").strip()

if not url:
    raise ValueError("âŒ Project_URL environment variable is missing or empty.")
if not key:
    raise ValueError("âŒ Secret_keys environment variable is missing or empty. (RLS ìš°íšŒë¥¼ ìœ„í•´ Service Role Keyê°€ í•„ìš”í•©ë‹ˆë‹¤)")

# Supabase í´ë¼ì´ì–¸íŠ¸ ìƒì„± (Secret Keyë¥¼ ì‚¬ìš©í•˜ì—¬ ëª¨ë“  í…Œì´ë¸”ì— ëŒ€í•œ CRUD ê¶Œí•œì„ í™•ë³´)
try:
    supabase: Client = create_client(url, key)
except Exception as e:
    raise RuntimeError(f"âŒ Supabase í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")


import pandas as pd
from datetime import datetime, timedelta

def load_toss_data_from_supabase():
    """
    Supabaseì—ì„œ ê°€ì¥ ìµœê·¼ ìˆ˜ì§‘ëœ ë‚ ì§œì˜ ë°ì´í„°ë¥¼ ë¡œë“œí•˜ì—¬ DataFrameìœ¼ë¡œ ë°˜í™˜í•©ë‹ˆë‹¤.
    """
    try:
        # 1. ê°€ì¥ ìµœê·¼ ìˆ˜ì§‘ëœ ë‚ ì§œ í™•ì¸
        res = supabase.table("toss_realtime_top100") \
            .select("collected_at") \
            .order("collected_at", desc=True) \
            .limit(1) \
            .execute()

        if not res.data:
            print("ğŸš¨ Supabaseì— ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return None

        latest_timestamp = res.data[0]['collected_at']
        # 'T' ë˜ëŠ” ê³µë°±ìœ¼ë¡œ ë¶„ë¦¬í•˜ì—¬ ë‚ ì§œ ë¶€ë¶„ë§Œ ì¶”ì¶œ (Timestamp ëŒ€ì‘)
        target_date = latest_timestamp.replace('T', ' ').split(' ')[0]
        print(f"ğŸ“… ê°€ì¥ ìµœê·¼ ë°ì´í„° ë‚ ì§œ: {target_date} (ë°ì´í„° ë¡œë“œ ì¤‘...)")

        # í•´ë‹¹ ë‚ ì§œì˜ ë°ì´í„° ë²”ìœ„ ì„¤ì • (UTC ê¸°ì¤€ì¼ ìˆ˜ ìˆìœ¼ë¯€ë¡œ í•˜ë£¨ë¥¼ í¬í•¨)
        start_date = target_date
        end_date_dt = datetime.strptime(target_date, "%Y-%m-%d") + timedelta(days=1)
        end_date = end_date_dt.strftime("%Y-%m-%d")

        # 2. í•´ë‹¹ ë‚ ì§œ ë°ì´í„° ì¿¼ë¦¬
        response = supabase.table("toss_realtime_top100") \
            .select("*") \
            .gte("collected_at", start_date) \
            .lt("collected_at", end_date) \
            .execute()

        if not response.data:
            print(f"ğŸš¨ {target_date} ë‚ ì§œì˜ ë°ì´í„°ë¥¼ ê°€ì ¸ì˜¤ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
            return None

        df = pd.DataFrame(response.data)

        # 3. ë°ì´í„° ì „ì²˜ë¦¬
        # ë§¤ìˆ˜(buy)ëŠ” ì–‘ìˆ˜, ë§¤ë„(sell)ëŠ” ìŒìˆ˜ë¡œ ë³€í™˜
        df['final_amount'] = df.apply(lambda x: x['amount'] if x['ranking_type'] == 'buy' else -x['amount'], axis=1)

        # í•„ìš”í•œ ì»¬ëŸ¼ë§Œ ì„ íƒ ë° ì´ë¦„ ë³€ê²½
        df_processed = df[['investor', 'stock_name', 'stock_code', 'final_amount', 'collected_at']].copy()
        
        # ì¤‘ë³µ ì œê±°: ë™ì¼ íˆ¬ìì, ì¢…ëª©, ë§¤ë§¤íƒ€ì…ì— ëŒ€í•´ ê°€ì¥ ë§ˆì§€ë§‰ì— ìˆ˜ì§‘ëœ ë°ì´í„°ë§Œ ì‚¬ìš©
        # (ì‹¤ì‹œê°„ í¬ë¡¤ë§ì´ ëˆ„ì ë  ê²½ìš° ìµœì‹  ìŠ¤ëƒ…ìƒ·ì„ ì‚¬ìš©í•˜ê¸° ìœ„í•¨)
        df_dedup = df.sort_values(by=['investor', 'stock_code', 'collected_at']) \
            .drop_duplicates(subset=['investor', 'stock_code', 'ranking_type'], keep='last')

        # ë§¤ìˆ˜/ë§¤ë„ í•©ì‚° (ê°™ì€ ì¢…ëª©ì— ëŒ€í•´ ë§¤ìˆ˜/ë§¤ë„ ëª¨ë‘ ìˆì„ ìˆ˜ ìˆìŒ)
        df_total = df_dedup.groupby(['investor', 'stock_name', 'stock_code'], as_index=False)['final_amount'].sum()
        
        # ì»¬ëŸ¼ëª… ë§¤í•‘ (ê¸°ì¡´ ë¡œì§ê³¼ì˜ í˜¸í™˜ì„±ì„ ìœ„í•´)
        df_total.rename(columns={
            'investor': 'íˆ¬ìì', 
            'stock_name': 'ì¢…ëª©ëª…', 
            'stock_code': 'ì¢…ëª©ì½”ë“œ', 
            'final_amount': 'ê¸ˆì•¡'
        }, inplace=True)

        print(f"âœ… Supabase ë°ì´í„° ë¡œë“œ ë° í†µí•© ì™„ë£Œ: {len(df_total)}ê±´ (ê¸°ì¤€ì¼: {target_date})")
        return df_total

    except Exception as e:
        print(f"ğŸš¨ Supabase ë°ì´í„° ë¡œë“œ ì¤‘ ì—ëŸ¬ ë°œìƒ: {e}")
        return None

def load_etf_pdf_from_supabase():
    """
    Supabaseì˜ etf_pdf í…Œì´ë¸”ì—ì„œ ë°ì´í„°ë¥¼ ë¡œë“œí•˜ì—¬ DataFrameìœ¼ë¡œ ë°˜í™˜í•©ë‹ˆë‹¤.
    ë°ì´í„°ê°€ ë§ì„ ê²½ìš° í˜ì´ì§€ë„¤ì´ì…˜ì„ í†µí•´ ëª¨ë‘ ê°€ì ¸ì˜µë‹ˆë‹¤.
    """
    try:
        all_data = []
        limit = 1000
        offset = 0
        
        print("â³ Supabase ETF PDF ë°ì´í„° ë¡œë“œ ì¤‘...", end='', flush=True)
        
        while True:
            # í˜ì´ì§€ë„¤ì´ì…˜: offsetë¶€í„° limitë§Œí¼ ê°€ì ¸ì˜¤ê¸°
            response = supabase.table("ETF_PDF").select("*").range(offset, offset + limit - 1).execute()
            
            if not response.data:
                break
                
            all_data.extend(response.data)
            
            # ê°€ì ¸ì˜¨ ë°ì´í„°ê°€ limitë³´ë‹¤ ì ìœ¼ë©´ ë” ì´ìƒ ë°ì´í„°ê°€ ì—†ë‹¤ëŠ” ëœ»
            if len(response.data) < limit:
                break
            
            offset += limit
            print(".", end='', flush=True)

        print(f"\nâœ… Supabase ETF PDF ë°ì´í„° ë¡œë“œ ì™„ë£Œ: {len(all_data)}ê±´")

        if not all_data:
            print("ğŸš¨ Supabaseì˜ 'ETF_PDF' í…Œì´ë¸”ì— ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return None

        df_pdf = pd.DataFrame(all_data)
        
        # ì»¬ëŸ¼ëª… í†µì¼ (DB ì»¬ëŸ¼ëª… -> ë¡œì§ìš© ì»¬ëŸ¼ëª…)
        column_mapping = {
            'etf_code': 'ETFì¢…ëª©ì½”ë“œ',
            'etf_name': 'ETFì¢…ëª©ëª…',
            'holdings_code': 'êµ¬ì„±ì¢…ëª©ì½”ë“œ',
            'holdings_name': 'êµ¬ì„±ì¢…ëª©ëª…',
            'holdings_weight': 'êµ¬ì„±ë¹„ì¤‘(%)'
        }
        
        # ì¡´ì¬í•˜ëŠ” ì»¬ëŸ¼ë§Œ ë³€ê²½
        existing_mapping = {k: v for k, v in column_mapping.items() if k in df_pdf.columns}
        if existing_mapping:
            df_pdf.rename(columns=existing_mapping, inplace=True)

        # ì¶”ê°€ì ì¸ ì»¬ëŸ¼ëª… í˜¸í™˜ ì²˜ë¦¬ (ì‹œê°€ì´ì•¡ê¸°ì¤€êµ¬ì„±ë¹„ìœ¨ ë“±)
        if 'ì‹œê°€ì´ì•¡ê¸°ì¤€êµ¬ì„±ë¹„ìœ¨' in df_pdf.columns:
            df_pdf.rename(columns={'ì‹œê°€ì´ì•¡ê¸°ì¤€êµ¬ì„±ë¹„ìœ¨': 'êµ¬ì„±ë¹„ì¤‘(%)'}, inplace=True)
        
        if 'êµ¬ì„±ë¹„ì¤‘(%)' not in df_pdf.columns:
            print(f"âš ï¸ 'êµ¬ì„±ë¹„ì¤‘(%)' ì»¬ëŸ¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. (ì»¬ëŸ¼: {list(df_pdf.columns)})")

        df_pdf['êµ¬ì„±ë¹„ì¤‘(%)'] = pd.to_numeric(df_pdf['êµ¬ì„±ë¹„ì¤‘(%)'], errors='coerce').fillna(0)
        
        # ì¢…ëª©ì½”ë“œ í¬ë§·íŒ… (6ìë¦¬ ë¬¸ìì—´)
        if 'ETFì¢…ëª©ì½”ë“œ' in df_pdf.columns:
            df_pdf['ETFì¢…ëª©ì½”ë“œ'] = df_pdf['ETFì¢…ëª©ì½”ë“œ'].astype(str).str.zfill(6)
        if 'êµ¬ì„±ì¢…ëª©ì½”ë“œ' in df_pdf.columns:
            df_pdf['êµ¬ì„±ì¢…ëª©ì½”ë“œ'] = df_pdf['êµ¬ì„±ì¢…ëª©ì½”ë“œ'].astype(str).str.zfill(6)

        return df_pdf

    except Exception as e:
        print(f"ğŸš¨ Supabase ETF PDF ë¡œë“œ ì¤‘ ì—ëŸ¬ ë°œìƒ: {e}")
        return None

def save_score_to_supabase(df):
    """
    ê³„ì‚°ëœ YG Score ê²°ê³¼ë¥¼ Supabase 'score' í…Œì´ë¸”ì— ì €ì¥(Upsert)í•©ë‹ˆë‹¤.
    ëª¨ë“  ë°ì´í„°ë¥¼ ê°•ì œë¡œ ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤.
    """
    try:
        if df is None or df.empty:
            print("âš ï¸ ì €ì¥í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return

        # 1. ì €ì¥í•  ë°ì´í„° ì¤€ë¹„ (ì»¬ëŸ¼ ë§¤í•‘)
        df_new = df.copy()
        df_new.rename(columns={
            'ETFì¢…ëª©ì½”ë“œ': 'etf_code',
            'ETFì¢…ëª©ëª…': 'etf_name',
            'YG_SCORE_í•©ê³„': 'total_score',
            'YG_SCORE_ì™¸êµ­ì¸': 'foreign_score',
            'YG_SCORE_ê¸°ê´€': 'institution_score',
            'ì¢…ëª©ìˆ˜': 'holdings_count'
        }, inplace=True)
        
        # 2. ì—…ë¡œë“œí•  ë°ì´í„° êµ¬ì„±
        # [ìˆ˜ì •] í•œêµ­ ì‹œê°„(KST) ê¸°ì¤€ Timestamp ì‚¬ìš©
        kst_now = datetime.utcnow() + timedelta(hours=9)
        current_time = kst_now.isoformat()
        
        upsert_cols = ['etf_code', 'etf_name', 'total_score', 'foreign_score', 'institution_score', 'holdings_count', 'updated_at']
        
        # updated_at ì¶”ê°€
        df_new['updated_at'] = current_time
        
        # í•„ìš”í•œ ì»¬ëŸ¼ë§Œ ì„ íƒí•˜ì—¬ dict ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜
        data_to_upsert = df_new[upsert_cols].to_dict(orient='records')

        # 3. ë°ì´í„° ì €ì¥ (ì „ì²´ Upsert)
        # ë°ì´í„°ê°€ ë§ì„ ê²½ìš° Supabase ìš”ì²­ ì œí•œì— ê±¸ë¦´ ìˆ˜ ìˆìœ¼ë¯€ë¡œ 1000ê°œì”© ë¶„í•  ì²˜ë¦¬
        batch_size = 1000
        total_count = len(data_to_upsert)
        
        print(f"ğŸš€ ì´ {total_count}ê±´ ë°ì´í„° ì €ì¥ì„ ì‹œì‘í•©ë‹ˆë‹¤...")
        
        for i in range(0, total_count, batch_size):
            batch = data_to_upsert[i:i+batch_size]
            res = supabase.table("score").upsert(batch).execute()
            print(f"   - {i} ~ {i+len(batch)}ê±´ ì €ì¥ ì™„ë£Œ")
            
        print(f"âœ… Supabase 'score' í…Œì´ë¸” ì „ì²´ ì—…ë°ì´íŠ¸ ì™„ë£Œ: {total_count}ê±´")

    except Exception as e:
        print(f"ğŸš¨ Supabase ì €ì¥ ì¤‘ ì—ëŸ¬ ë°œìƒ: {e}")

def delete_old_scores():
    """
    score í…Œì´ë¸”ì—ì„œ ì˜¤ëŠ˜(KST ê¸°ì¤€) ì´ì „ì˜ ë°ì´í„°ë¥¼ ëª¨ë‘ ì‚­ì œí•©ë‹ˆë‹¤.
    ì¦‰, ì‹¤í–‰ì¼ ë‹¹ì¼ì˜ ë°ì´í„°ë§Œ ë‚¨ê¹ë‹ˆë‹¤.
    """
    try:
        # KST ê¸°ì¤€ ì˜¤ëŠ˜ ì‹œì‘ ì‹œê°„ ê³„ì‚°
        now_utc = datetime.utcnow()
        now_kst = now_utc + timedelta(hours=9)
        today_start_kst = now_kst.replace(hour=0, minute=0, second=0, microsecond=0)
        
        # ë¹„êµë¥¼ ìœ„í•´ UTCë¡œ ë³€í™˜ (updated_atì€ UTCë¡œ ì €ì¥ë¨)
        # [ìˆ˜ì •] save_score_to_supabaseê°€ KST ISO í¬ë§·ìœ¼ë¡œ ì €ì¥í•˜ë¯€ë¡œ, thresholdë„ KST ISO í¬ë§·ìœ¼ë¡œ ì„¤ì •
        threshold_str = today_start_kst.isoformat()

        # ì‚­ì œ ì¿¼ë¦¬: updated_at < threshold
        response = supabase.table("score").delete().lt("updated_at", threshold_str).execute()
        
        deleted_count = len(response.data) if response.data else 0
        if deleted_count > 0:
            print(f"ğŸ§¹ ì§€ë‚œ Score ë°ì´í„° ì‚­ì œ ì™„ë£Œ: {deleted_count}ê±´ (ê¸°ì¤€: {today_start_kst.strftime('%Y-%m-%d')} KST ì´ì „)")
            
    except Exception as e:
        print(f"ğŸš¨ ì§€ë‚œ ë°ì´í„° ì‚­ì œ ì˜¤ë¥˜: {e}")
